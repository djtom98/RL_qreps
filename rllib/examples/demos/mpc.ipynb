{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from rllib.agent import MPCAgent\n",
    "from rllib.model import AbstractModel\n",
    "from rllib.reward.utilities import tolerance\n",
    "from rllib.environment import SystemEnvironment\n",
    "from rllib.environment.systems import InvertedPendulum, GaussianNoiseSystem\n",
    "from rllib.util.rollout import rollout_agent\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define reward and dynamic model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class PendulumSparseReward(AbstractModel):\n",
    "    \"\"\"Reward for Inverted Pendulum.\"\"\"\n",
    "\n",
    "    def __init__(self, action_cost=0):\n",
    "        super().__init__(dim_state=(2,), dim_action=(1,), model_kind=\"rewards\")\n",
    "        self.action_cost = action_cost\n",
    "        self.reward_offset = 0\n",
    "\n",
    "    def forward(self, state, action, next_state):\n",
    "        \"\"\"See `abstract_reward.forward'.\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.get_default_dtype())\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action, dtype=torch.get_default_dtype())\n",
    "\n",
    "        cos_angle = torch.cos(state[..., 0])\n",
    "        velocity = state[..., 1]\n",
    "\n",
    "        angle_tolerance = tolerance(cos_angle, lower=0.95, upper=1.0, margin=0.1)\n",
    "        velocity_tolerance = tolerance(velocity, lower=-0.5, upper=0.5, margin=0.5)\n",
    "        state_cost = angle_tolerance * velocity_tolerance\n",
    "\n",
    "        action_tolerance = tolerance(action[..., 0], lower=-0.1, upper=0.1, margin=0.1)\n",
    "        action_cost = self.action_cost * (action_tolerance - 1)\n",
    "\n",
    "        cost = state_cost + action_cost\n",
    "\n",
    "        return cost.unsqueeze(-1), torch.zeros(1)\n",
    "\n",
    "\n",
    "class PendulumDenseReward(AbstractModel):\n",
    "    \"\"\"Reward for Inverted Pendulum.\"\"\"\n",
    "\n",
    "    def __init__(self, action_cost=0.0):\n",
    "        super().__init__(dim_state=(2,), dim_action=(1,), model_kind=\"rewards\")\n",
    "        self.action_cost = action_cost\n",
    "        self.reward_offset = 0\n",
    "\n",
    "    def forward(self, state, action, next_state):\n",
    "        \"\"\"See `abstract_reward.forward'.\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.get_default_dtype())\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action, dtype=torch.get_default_dtype())\n",
    "\n",
    "        cos_angle = 1 - torch.cos(state[..., 0])\n",
    "        state_cost = cos_angle ** 2\n",
    "        action_cost = self.action_cost * (action ** 2).sum(-1)\n",
    "\n",
    "        return -(action_cost + state_cost), torch.tensor(0.0)\n",
    "\n",
    "\n",
    "class PendulumModel(AbstractModel):\n",
    "    \"\"\"Pendulum Model.\n",
    "\n",
    "    Torch implementation of a pendulum model using euler forwards integration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, mass, length, friction, step_size=1 / 80, noise: MultivariateNormal = None\n",
    "    ):\n",
    "        super().__init__(dim_state=(2,), dim_action=(1,))\n",
    "        self.mass = mass\n",
    "        self.length = length\n",
    "        self.friction = friction\n",
    "        self.step_size = step_size\n",
    "        self.noise = noise\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Get next-state distribution.\"\"\"\n",
    "        # Physical dynamics\n",
    "        action = torch.clamp(action, -1.0, 1.0)\n",
    "        mass = self.mass\n",
    "        gravity = 9.81\n",
    "        length = self.length\n",
    "        friction = self.friction\n",
    "        inertia = mass * length ** 2\n",
    "        dt = self.step_size\n",
    "\n",
    "        angle, angular_velocity = torch.split(state, 1, dim=-1)\n",
    "        for _ in range(1):\n",
    "            x_ddot = (\n",
    "                (gravity / length) * torch.sin(angle)\n",
    "                + action * (1 / inertia)\n",
    "                - (friction / inertia) * angular_velocity\n",
    "            )\n",
    "\n",
    "            angle = angle + dt * angular_velocity\n",
    "            angular_velocity = angular_velocity + dt * x_ddot\n",
    "\n",
    "        next_state = torch.cat((angle, angular_velocity), dim=-1)\n",
    "\n",
    "        if self.noise is None:\n",
    "            return next_state, torch.zeros(1)\n",
    "        else:\n",
    "            return next_state + self.noise.mean, self.noise.covariance_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "action_cost=0.1\n",
    "sparse_reward = False\n",
    "solver = \"CEMShooting\"\n",
    "sparse_reward_model = PendulumSparseReward(action_cost=action_cost)\n",
    "dense_reward_model = PendulumDenseReward(action_cost=action_cost)\n",
    "reward_model = sparse_reward_model if sparse_reward else dense_reward_model\n",
    "dynamical_model =  PendulumModel(mass=0.3, length=0.5, friction=0.005, step_size=1 / 80)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "initial_distribution = torch.distributions.Uniform(\n",
    "    torch.tensor([np.pi, -0.0]), torch.tensor([np.pi, +0.0])\n",
    ")\n",
    "environment = SystemEnvironment(\n",
    "    InvertedPendulum(mass=0.3, length=0.5, friction=0.005, step_size=1 / 80),\n",
    "    reward=reward_model,\n",
    "    initial_state=initial_distribution.sample,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Agent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.15s/it]\n"
     ]
    }
   ],
   "source": [
    "agent = MPCAgent.default(\n",
    "        environment=environment,\n",
    "        mpc_solver_name=\"MPPIShooting\",\n",
    "        dynamical_model=dynamical_model,\n",
    "        reward_model=reward_model,\n",
    "        exploration_episodes=0,\n",
    "        horizon=50,\n",
    "    )\n",
    "\n",
    "rollout_agent(agent=agent, environment=environment, max_steps=400, num_episodes=1, render=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
