from typing import Any, Iterable, Optional, Sequence, Tuple, TypeVar, Union

import numpy as np
import torch.nn as nn
from torch import Size, Tensor

Module = TypeVar("Module", bound=nn.Module)

def deep_copy_module(module: Module) -> Module: ...

class Swish(nn.Module):
    def forward(self, *args: Tensor, **kwargs: Any) -> Tensor: ...

class Mish(nn.Module):
    def forward(self, *args: Tensor, **kwargs: Any) -> Tensor: ...

def parse_nonlinearity(non_linearity: str) -> nn.Module: ...
def parse_layers(
    layers: Sequence[int], in_dim: Tuple, non_linearity: str
) -> Tuple[nn.Sequential, int]: ...
def update_parameters(
    target_module: nn.Module, new_module: nn.Module, tau: float = ...
) -> None: ...
def count_vars(module: nn.Module) -> int: ...
def zero_bias(module: nn.Module) -> None: ...
def init_head_bias(
    module: nn.Module, offset: float = ..., delta: float = ...
) -> None: ...
def init_head_weight(
    module: nn.Module, mean_weight: float = ..., scale_weight: float = ...
) -> None: ...
def inverse_softplus(x: Tensor) -> Tensor: ...

class TileCode(nn.Module):
    tiles: Tensor
    bins: Tensor
    num_outputs: int
    extra_dims: int
    one_hot: bool
    def __init__(
        self,
        low: Sequence[float],
        high: Sequence[float],
        bins: int,
        one_hot: bool = ...,
    ) -> None: ...
    def _tuple_to_int(self, tuple_: Union[Tensor, Tuple[Tensor]]) -> Tensor: ...
    def forward(self, *args: Tensor, **kwargs: Any) -> Tensor: ...

def digitize(tensor: Tensor, bin_boundaries: Tensor) -> Tensor: ...

class OneHotEncode(nn.Module):
    num_classes: int
    extra_dim: int
    def __init__(self, num_classes: int) -> None: ...
    def forward(self, *args: Tensor, **kwargs: Any) -> Tensor: ...

def one_hot_encode(tensor: Tensor, num_classes: int) -> Tensor: ...
def reverse_cumsum(tensor: Tensor, dim: int = ...) -> Tensor: ...
def reverse_cumprod(tensor: Tensor, dim: int = ...) -> Tensor: ...
def get_batch_size(tensor: Tensor, base_shape: Union[Size, Tuple]) -> Tuple[int]: ...
def random_tensor(
    discrete: bool, dim: int, batch_size: Optional[int] = ...
) -> Tensor: ...
def repeat_along_dimension(array: Tensor, number: int, dim: int = ...) -> Tensor: ...
def torch_quadratic(array: Tensor, matrix: Tensor) -> Tensor: ...
def freeze_parameters(module: nn.Module) -> None: ...
def unfreeze_parameters(module: nn.Module) -> None: ...
def stop_learning(module: nn.Module) -> None: ...
def resume_learning(module: nn.Module) -> None: ...

class DisableGradient(object):
    modules: Iterable[nn.Module]
    def __init__(self, *modules: nn.Module) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...

class EnableGradient(object):
    modules: Iterable[nn.Module]
    def __init__(self, *modules: nn.Module) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...

class DisableOptimizer(object):
    modules: Iterable[nn.Module]
    def __init__(self, *modules: nn.Module) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...
